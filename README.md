# open-llm-prompt-security
Open standard for LLM prompt injection, indirect instruction, and data exfiltration risk.
## Why this exists
Large Language Models introduce a new attack surface that traditional
application security, DLP, and IAM controls do not fully address.

This project aims to define **what “good” looks like** for securing LLM prompts
and context — in a way that is usable by security, risk, compliance, and audit teams.

## Scope
- Prompt injection and instruction override
- Indirect prompt injection via documents, RAG, and tools
- Data exfiltration and unintended disclosure
- Control objectives, control statements, and audit evidence expectations mapped with risk

## Status
Early draft. Structure-first. No tooling dependencies.
